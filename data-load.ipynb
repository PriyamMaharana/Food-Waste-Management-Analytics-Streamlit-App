{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, os, json, glob\n",
    "from sqlalchemy import create_engine, text\n",
    "from sqlalchemy.exc import ProgrammingError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column names and SQL types extracted from all CSV files and saved to column.json\n"
     ]
    }
   ],
   "source": [
    "def map_dtype_to_sql(dtype, sample_values=None):\n",
    "    dtype = str(dtype)\n",
    "    if 'int' in dtype:\n",
    "        return 'int'\n",
    "    elif 'float' in dtype:\n",
    "        return 'int'  # or 'float' if needed\n",
    "    elif 'datetime' in dtype:\n",
    "        return 'datetime'\n",
    "    # Try to infer datetime from string/object\n",
    "    elif dtype == 'object' and sample_values is not None:\n",
    "        # Only check non-null values\n",
    "        sample_values = [v for v in sample_values if pd.notnull(v)]\n",
    "        success_count = 0\n",
    "        for v in sample_values[:10]:  # sample first 10 non-null entries\n",
    "            try:\n",
    "                pd.to_datetime(v, dayfirst=True)\n",
    "                success_count += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "        if success_count >= 8:  # 80%+ look like datetimes\n",
    "            return 'datetime'\n",
    "        else:\n",
    "            return 'string'\n",
    "    else:\n",
    "        return 'string'\n",
    "\n",
    "csv_files = glob.glob(\"*.csv\")\n",
    "col_dict = {}\n",
    "\n",
    "for file in csv_files:\n",
    "    try:\n",
    "        df = pd.read_csv(file, nrows=100)\n",
    "        filename_only = os.path.splitext(os.path.basename(file))[0]\n",
    "        filename_only = filename_only.replace(\"_cleaned\", \"\")\n",
    "        col_types = {}\n",
    "        for col, dtype in df.dtypes.items():\n",
    "            sample_values = df[col].dropna().astype(str).tolist()[:10]\n",
    "            col_types[col] = map_dtype_to_sql(dtype, sample_values)\n",
    "        col_dict[filename_only] = col_types\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "with open(\"column.json\", \"w\") as f:\n",
    "    json.dump(col_dict, f, indent=4)\n",
    "\n",
    "print(\"Column names and SQL types extracted from all CSV files and saved to column.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database food_db already exists!!\n"
     ]
    }
   ],
   "source": [
    "db_url = \"postgresql+psycopg2://postgres:1234@localhost:5432/postgres\"\n",
    "\n",
    "db_name = \"food_db\"\n",
    "\n",
    "engine = create_engine(db_url, isolation_level='AUTOCOMMIT')\n",
    "\n",
    "with engine.connect() as conn:\n",
    "    result = conn.execute(text(f\"SELECT 1 FROM pg_database WHERE datname = '{db_name}'\"))\n",
    "    exists = result.scalar()\n",
    "    if not exists:\n",
    "        conn.execute(text(f'CREATE DATABASE \"{db_name}\"'))\n",
    "        print(f\"Database {db_name} created..!!\")\n",
    "    else:\n",
    "        print(f\"Database {db_name} already exists!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating table claim_data...\n",
      "Creating table food_data...\n",
      "Creating table provider_data...\n",
      "Creating table receiver_data...\n",
      "Tables created successfully.\n"
     ]
    }
   ],
   "source": [
    "db_url = \"postgresql+psycopg2://postgres:1234@localhost:5432/food_db\"\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "with open('column.json') as f:\n",
    "    schemas = json.load(f)\n",
    "\n",
    "type_map = {\n",
    "    'int': 'INTEGER',\n",
    "    'string': 'VARCHAR',\n",
    "    'datetime': 'TIMESTAMP'\n",
    "}\n",
    "\n",
    "with engine.begin() as conn:\n",
    "    for table_name, columns in schemas.items():\n",
    "        # Force lowercase for table name\n",
    "        table_name_lc = table_name.lower()\n",
    "\n",
    "        # Build column definitions (lowercase)\n",
    "        col_defs = []\n",
    "        for col_name, col_type in columns.items():\n",
    "            col_name_lc = col_name.lower()\n",
    "            sql_type = type_map.get(col_type.lower(), 'VARCHAR')\n",
    "            if sql_type == 'VARCHAR':\n",
    "                sql_type = 'VARCHAR(255)'\n",
    "            col_defs.append(f\"{col_name_lc} {sql_type}\")  # no quotes\n",
    "\n",
    "        col_defs_str = \", \".join(col_defs)\n",
    "        create_table_sql = f'CREATE TABLE IF NOT EXISTS {table_name_lc} ({col_defs_str});'\n",
    "\n",
    "        print(f\"Creating table {table_name_lc}...\")\n",
    "        conn.execute(text(create_table_sql))\n",
    "\n",
    "print(\"Tables created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading claim_data_cleaned.csv into claim_data...\n",
      "✅ Data inserted into claim_data\n",
      "Loading food_data_cleaned.csv into food_data...\n",
      "✅ Data inserted into food_data\n",
      "Loading provider_data_cleaned.csv into provider_data...\n",
      "✅ Data inserted into provider_data\n",
      "Loading receiver_data_cleaned.csv into receiver_data...\n",
      "✅ Data inserted into receiver_data\n"
     ]
    }
   ],
   "source": [
    "db_url = \"postgresql+psycopg2://postgres:1234@localhost:5432/food_db\"\n",
    "engine = create_engine(db_url)\n",
    "\n",
    "with open('column.json') as f:\n",
    "    schemas = json.load(f)\n",
    "    \n",
    "file_to_table = {\n",
    "    \"claim_data_cleaned.csv\": \"claim_data\",\n",
    "    \"food_data_cleaned.csv\": \"food_data\",\n",
    "    \"provider_data_cleaned.csv\": \"provider_data\",\n",
    "    \"receiver_data_cleaned.csv\": \"receiver_data\"\n",
    "}\n",
    "\n",
    "# Loop through all tables in schema\n",
    "for csv_file, table_name in file_to_table.items():\n",
    "    print(f\"Loading {csv_file} into {table_name}...\")\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    \n",
    "    # Keep only columns in schema, in correct order\n",
    "    schema_cols = list(schemas[table_name].keys())\n",
    "    df = df[schema_cols]\n",
    "    \n",
    "    # Type conversion based on JSON schema\n",
    "    for col, sql_type in schemas[table_name].items():\n",
    "        if sql_type == 'datetime':\n",
    "            df[col] = pd.to_datetime(df[col], errors='coerce', dayfirst=True)\n",
    "        elif sql_type == 'int':\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')\n",
    "        else:\n",
    "            df[col] = df[col].astype(str)\n",
    "    \n",
    "    # Insert into SQL\n",
    "    df.to_sql(table_name, engine, if_exists='append', index=False)\n",
    "    print(f\"✅ Data inserted into {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
